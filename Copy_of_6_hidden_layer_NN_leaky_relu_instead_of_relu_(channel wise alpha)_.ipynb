{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1mtlhFQKhtQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.special import softmax as sm\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, num_features, num_samples,num_hidden1,num_hidden2,num_hidden3,num_hidden4,num_hidden5,num_hidden6 ,alpha, max_epochs, num_output, _EPSILON,alpha1,alpha2,alpha3,alpha4,alpha5,alpha6):\n",
    "        super().__init__()\n",
    "        self.num_features=num_features  # number of input nodes (features)\n",
    "        self.no_of_samples=num_samples\n",
    "        self.num_hidden1=num_hidden1  # number of hidden nodes for 1st hidden layer\n",
    "        self.num_hidden2=num_hidden2  # number of hidden nodes for 2nd hidden layer\n",
    "        self.num_hidden3=num_hidden3  # number of hidden nodes for 3rd hidden layer\n",
    "        self.num_hidden4=num_hidden4  # number of hidden nodes for 4th hidden layer\n",
    "        self.num_hidden5=num_hidden5  # number of hidden nodes for 5th hidden layer\n",
    "        self.num_hidden6=num_hidden6  # number of hidden nodes for 6th hidden layer\n",
    "        self.alpha1=alpha1*np.ones([self.num_hidden1,1])\n",
    "        self.alpha2=alpha2*np.ones([self.num_hidden2,1])\n",
    "        self.alpha3=alpha3*np.ones([self.num_hidden3,1])\n",
    "        self.alpha4=alpha4*np.ones([self.num_hidden4,1])\n",
    "        self.alpha5=alpha5*np.ones([self.num_hidden5,1])\n",
    "        self.alpha6=alpha6*np.ones([self.num_hidden6,1])\n",
    "        self.alpha=alpha  # learning rate\n",
    "        self.max_epochs=max_epochs # maximum number of epochs\n",
    "        self.num_output=num_output # number of output nodes\n",
    "        self._EPSILON=_EPSILON\n",
    "        self.loss = [] #list to store losses per 100 epochs \n",
    "        self.trainingaccur=[] # list to store training accuracy per 100 epochs \n",
    "        self.devaccur=[]\n",
    "        self.Weights_Input_to_H1=np.random.randn(self.num_hidden1, self.num_features)*(0.1)\n",
    "        self.Bias_Input_to_H1=np.zeros([self.num_hidden1,1])\n",
    "        self.Weights_H1_to_H2=np.random.randn(self.num_hidden2, self.num_hidden1)*(0.1)\n",
    "        self.Bias_H1_to_H2=np.zeros([self.num_hidden2,1])\n",
    "        self.Weights_H2_to_H3=np.random.randn(self.num_hidden3, self.num_hidden2)*(0.1)\n",
    "        self.Bias_H2_to_H3=np.zeros([self.num_hidden3,1])\n",
    "        self.Weights_H3_to_H4=np.random.randn(self.num_hidden4, self.num_hidden3)*(0.1)\n",
    "        self.Bias_H3_to_H4=np.zeros([self.num_hidden4,1])\n",
    "        self.Weights_H4_to_H5=np.random.randn(self.num_hidden5, self.num_hidden4)*(0.1)\n",
    "        self.Bias_H4_to_H5=np.zeros([self.num_hidden5,1])\n",
    "        self.Weights_H5_to_H6=np.random.randn(self.num_hidden6, self.num_hidden5)*(0.1)\n",
    "        self.Bias_H5_to_H6=np.zeros([self.num_hidden6,1])\n",
    "        self.Weights_H6_to_output=np.random.randn(self.num_output, self.num_hidden6)*(0.1)\n",
    "        self.Bias_H6_to_output=np.zeros([self.num_output,1])\n",
    "        self.dWeights_Input_to_H1=np.zeros([self.num_hidden1, self.num_features])\n",
    "        self.dBias_Input_to_H1=np.zeros([self.num_hidden1,1])\n",
    "        self.dWeights_H1_to_H2=np.zeros([self.num_hidden2, self.num_hidden1])\n",
    "        self.dBias_H1_to_H2=np.zeros([self.num_hidden2,1])\n",
    "        self.dWeights_H2_to_H3=np.zeros([self.num_hidden3, self.num_hidden2])\n",
    "        self.dBias_H2_to_H3=np.zeros([self.num_hidden3,1])\n",
    "        self.dWeights_H3_to_H4=np.zeros([self.num_hidden4, self.num_hidden3])\n",
    "        self.dBias_H3_to_H4=np.zeros([self.num_hidden4,1])\n",
    "        self.dWeights_H4_to_H5=np.zeros([self.num_hidden5, self.num_hidden4])\n",
    "        self.dBias_H4_to_H5=np.zeros([self.num_hidden5,1])\n",
    "        self.dWeights_H5_to_H6=np.zeros([self.num_hidden6, self.num_hidden5])\n",
    "        self.dBias_H5_to_H6=np.zeros([self.num_hidden6,1])\n",
    "        self.dWeights_H6_to_output=np.zeros([self.num_output, self.num_hidden6])\n",
    "        self.dBias_H6_to_output=np.zeros([self.num_output,1])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def relU(self,X):\n",
    "        return np.maximum(X,0)\n",
    "\n",
    "    def leakreLU(self,X,alpha):\n",
    "        return np.maximum(0, X) + alpha * np.minimum(0, X)\n",
    "      \n",
    "    \n",
    "    def deriv(self,X):\n",
    "        return np.where(X<=0,0,1)\n",
    "\n",
    "    def deriv_alpha(self,alpha,X):\n",
    "        return np.where(X<=0,X,0)\n",
    "    \n",
    "    def deriv_z(self,X,alpha):\n",
    "        return np.where(X<=0,alpha,1)  \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e=np.exp(x)\n",
    "        for i in range(e.shape[1]):\n",
    "            e[:,i]=e[:,i]/np.sum(e[:,i])\n",
    "        return e\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    # TODO: complete implementation for forward pass\n",
    "    def forward(self, X):\n",
    "        self.z1=np.dot((self.Weights_Input_to_H1),(X))+self.Bias_Input_to_H1\n",
    "        self.a1=self.leakreLU(self.z1,self.alpha1)\n",
    "        self.z2=np.dot((self.Weights_H1_to_H2),(self.a1))+self.Bias_H1_to_H2\n",
    "        self.a2=self.leakreLU(self.z2,self.alpha2)\n",
    "        self.z3=np.dot((self.Weights_H2_to_H3),(self.a2))+self.Bias_H2_to_H3\n",
    "        self.a3=self.leakreLU(self.z3,self.alpha3)\n",
    "        self.z4=np.dot((self.Weights_H3_to_H4),(self.a3))+self.Bias_H3_to_H4\n",
    "        self.a4=self.leakreLU(self.z4,self.alpha4)\n",
    "        self.z5=np.dot((self.Weights_H4_to_H5),(self.a4))+self.Bias_H4_to_H5\n",
    "        self.a5=self.leakreLU(self.z5,self.alpha5)\n",
    "        self.z6=np.dot((self.Weights_H5_to_H6),(self.a5))+self.Bias_H5_to_H6\n",
    "        self.a6=self.leakreLU(self.z6,self.alpha6)\n",
    "        self.z7=np.dot((self.Weights_H6_to_output),(self.a6))+self.Bias_H6_to_output\n",
    "        self.a7=self.softmax((self.z7))\n",
    "        return self.a7\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # TODO: complete implementation for backpropagation\n",
    "    # the following Numpy functions may be useful: np.dot, np.sum, np.tanh, numpy.ndarray.T\n",
    "    def backprop(self, X, t):\n",
    "\n",
    "        self.dz7=(self.a7.reshape(self.num_output,-1)-t.reshape(self.num_output,-1))/((self.num_output)*(X.shape[1])*0.1)\n",
    "        self.dBias_H6_to_output=np.sum(self.dz7,axis=1,keepdims=True)\n",
    "        self.dWeights_H6_to_output=np.dot((self.dz7),self.a6.T)\n",
    "        self.dz6=np.dot(self.Weights_H6_to_output.T,self.dz7)* (self.deriv_z(self.z6,self.alpha6))\n",
    "        self.dBias_H5_to_H6=np.sum(self.dz6,axis=1,keepdims=True)\n",
    "        self.dWeights_H5_to_H6=np.dot((self.dz6),(self.a5.T))\n",
    "        self.dalpha6=np.sum(((np.dot(self.Weights_H6_to_output.T,self.dz7)) * (self.deriv_alpha(self.alpha6,self.z6))),axis=1,keepdims=True)\n",
    "        self.dz5=(np.dot(self.Weights_H5_to_H6.T,self.dz6)) * (self.deriv_z(self.z5,self.alpha5))\n",
    "        self.dBias_H4_to_H5=np.sum(self.dz5,axis=1,keepdims=True)\n",
    "        self.dWeights_H4_to_H5=np.dot((self.dz5),(self.a4.T))\n",
    "        self.dalpha5=np.sum(((np.dot(self.Weights_H5_to_H6.T,self.dz6)) * (self.deriv_alpha(self.alpha5,self.z5))),axis=1,keepdims=True)\n",
    "        self.dz4=(np.dot(self.Weights_H4_to_H5.T,self.dz5)) * (self.deriv_z(self.z4,self.alpha4))\n",
    "        self.dBias_H3_to_H4=np.sum(self.dz4,axis=1,keepdims=True)\n",
    "        self.dWeights_H3_to_H4=np.dot((self.dz4),(self.a3.T))\n",
    "        self.dalpha4=np.sum(((np.dot(self.Weights_H4_to_H5.T,self.dz5)) * (self.deriv_alpha(self.alpha4,self.z4))),axis=1,keepdims=True)\n",
    "        self.dz3=(np.dot(self.Weights_H4_to_H5.T,self.dz4)) * (self.deriv_z(self.z3,self.alpha3))\n",
    "        self.dBias_H2_to_H3=np.sum(self.dz3,axis=1,keepdims=True)\n",
    "        self.dWeights_H2_to_H3=np.dot((self.dz3),(self.a2.T))\n",
    "        self.dalpha3=np.sum(((np.dot(self.Weights_H3_to_H4.T,self.dz4)) * (self.deriv_alpha(self.alpha3,self.z3))),axis=1,keepdims=True)\n",
    "        self.dz2=(np.dot(self.Weights_H3_to_H4.T,self.dz3)) * (self.deriv_z(self.z2,self.alpha2))\n",
    "        self.dBias_H1_to_H2=np.sum(self.dz2,axis=1,keepdims=True)\n",
    "        self.dWeights_H1_to_H2=np.dot((self.dz2),(self.a1.T))\n",
    "        self.dalpha2=np.sum(((np.dot(self.Weights_H2_to_H3.T,self.dz3)) * (self.deriv_alpha(self.alpha2,self.z2))),axis=1,keepdims=True)\n",
    "        self.dz1=(np.dot(self.Weights_H1_to_H2.T,self.dz2)) * (self.deriv_z(self.z1,self.alpha1))\n",
    "        self.dBias_Input_to_H1=np.sum(self.dz1,axis=1,keepdims=True)\n",
    "        self.dWeights_Input_to_H1=np.dot((self.dz1),X.T)\n",
    "        self.dalpha1=np.sum(((np.dot(self.Weights_H1_to_H2.T,self.dz2)) * (self.deriv_alpha(self.alpha1,self.z1))),axis=1,keepdims=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "              \n",
    "                        \n",
    "                \n",
    "      \n",
    "        \n",
    "        \n",
    "    \n",
    "    #TODO: complete implementation for fitting data, and change the existing code if needed\n",
    "    def fit(self, x_train_data, y_train_data,x_dev_data,y_dev_data):\n",
    "       \n",
    "        \n",
    "        \n",
    "        for step in range(self.max_epochs):\n",
    "            self.forward(x_train_data)\n",
    "            self.backprop(x_train_data, y_train_data)\n",
    "            self.Bias_H6_to_output=self.Bias_H6_to_output-((self.alpha)*(self.dBias_H6_to_output))\n",
    "            self.Weights_H6_to_output=self.Weights_H6_to_output-((self.alpha)*(self.dWeights_H6_to_output))\n",
    "            self.Bias_H5_to_H6=self.Bias_H5_to_H6-((self.alpha)*(self.dBias_H5_to_H6))\n",
    "            self.Weights_H5_to_H6=self.Weights_H5_to_H6-((self.alpha)*(self.dWeights_H5_to_H6))\n",
    "            self.Bias_H4_to_H5=self.Bias_H4_to_H5-((self.alpha)*(self.dBias_H4_to_H5))\n",
    "            self.Weights_H4_to_H5=self.Weights_H4_to_H5-((self.alpha)*(self.dWeights_H4_to_H5))\n",
    "            self.Bias_H3_to_H4=self.Bias_H3_to_H4-((self.alpha)*(self.dBias_H3_to_H4))\n",
    "            self.Weights_H3_to_H4=self.Weights_H3_to_H4-((self.alpha)*(self.dWeights_H3_to_H4))\n",
    "            self.Bias_H2_to_H3=self.Bias_H2_to_H3-((self.alpha)*(self.dBias_H2_to_H3))\n",
    "            self.Weights_H2_to_H3=self.Weights_H2_to_H3-((self.alpha)*(self.dWeights_H2_to_H3))\n",
    "            self.Bias_H1_to_H2=self.Bias_H1_to_H2-((self.alpha)*(self.dBias_H1_to_H2))\n",
    "            self.Weights_H1_to_H2=self.Weights_H1_to_H2-((self.alpha)*(self.dWeights_H1_to_H2))\n",
    "            self.Bias_Input_to_H1=self.Bias_Input_to_H1-((self.alpha)*(self.dBias_Input_to_H1))\n",
    "            self.Weights_Input_to_H1=self.Weights_Input_to_H1-((self.alpha)*(self.dWeights_Input_to_H1))\n",
    "            self.alpha1=self.alpha1-((self.alpha)*(self.dalpha1))\n",
    "            self.alpha2=self.alpha2-((self.alpha)*(self.dalpha2))\n",
    "            self.alpha3=self.alpha3-((self.alpha)*(self.dalpha3))\n",
    "            self.alpha4=self.alpha4-((self.alpha)*(self.dalpha4))\n",
    "            self.alpha5=self.alpha5-((self.alpha)*(self.dalpha5))\n",
    "            self.alpha6=self.alpha6-((self.alpha)*(self.dalpha6))\n",
    "            \n",
    "      \n",
    "\n",
    "            if step % 100 == 0:\n",
    "                self.CCloss=log_loss(np.transpose(y_train_data),np.transpose(self.a7),eps=self._EPSILON,normalize=True)\n",
    "                self.trainingaccuracy=accuracy_score(np.argmax(y_train_data,axis=0),np.argmax(self.forward(x_train_data),axis=0))\n",
    "                self.devaccuracy=accuracy_score(np.argmax(y_dev_data,axis=0),np.argmax(self.forward(x_dev_data),axis=0))\n",
    "                print(f'step: {step},  loss: {self.CCloss:3.150f}') \n",
    "                print(accuracy_score(np.argmax(y_train_data,axis=0),np.argmax(self.forward(x_train_data),axis=0)))\n",
    "                print(accuracy_score(np.argmax(y_dev_data,axis=0),np.argmax(self.forward(x_dev_data),axis=0)))\n",
    "                self.loss.append(self.CCloss)\n",
    "                self.trainingaccur.append(self.trainingaccuracy)\n",
    "                self.devaccur.append(self.devaccuracy)\n",
    "                print(NN.alpha1,NN.dalpha1)\n",
    "                print(NN.alpha2,NN.dalpha2)\n",
    "                print(NN.alpha3,NN.dalpha3)\n",
    "                print(NN.alpha4,NN.dalpha4)\n",
    "                print(NN.alpha5,NN.dalpha5)\n",
    "                print(NN.alpha6,NN.dalpha6)\n",
    "               \n",
    "                \n",
    "            \n",
    "\n",
    "                \n",
    "              \n",
    "            \n",
    "            \n",
    "    def predict(self,X,y=None):\n",
    "        self.forward(X)\n",
    "        if(self.num_output>1):\n",
    "            y_hat=np.argmax(self.a7, axis=0)\n",
    "            temp=accuracy_score(y_hat,y)\n",
    "        else:\n",
    "            y_hat=np.where(self.a7>0.5,1,0)\n",
    "            temp=accuracy_score(y_hat,y)\n",
    "        return temp,y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJwVBH02-OUV"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_QHijXHKtnp"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "mnist = load_digits()\n",
    "X=mnist.data\n",
    "Y=mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQNCZFrVKxS-",
    "outputId": "f83220e9-9682-4229-a58f-fe578f9fac11"
   },
   "outputs": [],
   "source": [
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMOtwBu9K1KI"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X_train, Y_train, test_size=0.11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joIg56FeK3Oq"
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "scaler=sk.preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8JIeiVgK7Ea"
   },
   "outputs": [],
   "source": [
    "for a in range(X_train.shape[1]):\n",
    "  X_train[:,a]=scaler.fit_transform(X_train[:,a].reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqzdZR33LBGb"
   },
   "outputs": [],
   "source": [
    "Y_train=np.array(pd.get_dummies(np.array(Y_train)))\n",
    "Y_dev=np.array(pd.get_dummies(np.array(Y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjmbFZMpLCNb"
   },
   "outputs": [],
   "source": [
    "X_train=np.transpose(X_train)\n",
    "X_dev=np.transpose(X_dev)\n",
    "Y_train=np.transpose(Y_train)\n",
    "Y_dev=np.transpose(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlfiPcKFLF1L"
   },
   "outputs": [],
   "source": [
    "numHidden1 = 500 # number of hidden nodes\n",
    "numHidden2 = 500# number of hidden nodes\n",
    "numHidden3 = 500# number of hidden nodes\n",
    "numHidden4 = 500# number of hidden nodes\n",
    "numHidden5 = 500# number of hidden nodes\n",
    "numHidden6 = 500# number of hidden nodes\n",
    "num_features = X_train.shape[0]\n",
    "num_samples=X_train.shape[1]\n",
    "numOutput = Y_train.shape[0]\n",
    "max_epoches = 1000000\n",
    "alpha = 0.01\n",
    "epsilon=0.00000000001\n",
    "alpha1=-0.018\n",
    "alpha2=-0.129\n",
    "alpha3=-0.089\n",
    "alpha4=0.047\n",
    "alpha5=0.103\n",
    "alpha6=-0.011\n",
    "NN = NeuralNet(num_features,num_samples, numHidden1,numHidden2,numHidden3,numHidden4,numHidden5,numHidden6, alpha, max_epoches, numOutput,epsilon,alpha1,alpha2,alpha3,alpha4,alpha5,alpha6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UB4iTrCkLJex",
    "outputId": "ef463c44-66a7-452a-e76b-381d1363d71e"
   },
   "outputs": [],
   "source": [
    "NN.fit(X_train,Y_train,X_dev,Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weLuBBXWxjro"
   },
   "outputs": [],
   "source": [
    "NN.alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVpcqi2N1uOP",
    "outputId": "26a68306-9c09-4edf-9cb6-a4ae4921de2d"
   },
   "outputs": [],
   "source": [
    "accuracy_score((Y_test),np.argmax(NN.forward(X_test.T),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "RY8sjuvM1wT8",
    "outputId": "0efc177b-b1de-4012-cede-f4dc9647496c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_loss=range(0,len(NN.loss)*100,100)\n",
    "\n",
    "\n",
    "line1=plt.plot(x_loss,NN.loss,linestyle='-',label='training loss')  \n",
    "\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "legend = plt.legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "roydgq6w1yaQ",
    "outputId": "8381817a-7172-4211-c1df-2d12a40bd2b5"
   },
   "outputs": [],
   "source": [
    "x_training_accur=range(0,len(NN.trainingaccur)*100,100)\n",
    "x_devaccur=range(0,len(NN.devaccur)*100,100)\n",
    "\n",
    "line1=plt.plot(x_training_accur,NN.trainingaccur,linestyle='-',label='training accuracy') \n",
    "line2=plt.plot(x_devaccur,NN.devaccur,linestyle='-',label='dev accuracy')\n",
    "                              \n",
    "plt.title('Training and Dev Accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kicOGg0EBJrD"
   },
   "outputs": [],
   "source": [
    "NN.dalpha1=(((np.dot(NN.Weights_H1_to_H2.T,NN.dz2))*(NN.deriv_alpha(NN.z1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kU9rxmjECuT0",
    "outputId": "74d142f0-871a-4eef-9f08-89952a496f56"
   },
   "outputs": [],
   "source": [
    "NN.dalpha1[0][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(NN.alpha1,NN.dalpha1)\n",
    " print(NN.alpha2,NN.dalpha2)\n",
    " print(NN.alpha3,NN.dalpha3)\n",
    " print(NN.alpha4,NN.dalpha4)\n",
    " print(NN.alpha5,NN.dalpha5)\n",
    " print(NN.alpha6,NN.dalpha6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
